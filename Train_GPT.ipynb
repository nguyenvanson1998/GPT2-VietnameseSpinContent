{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8259b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 17:58:42.541839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"XRT_TPU_CONFIG\"] = \"localservice;0;localhost:51011\" \n",
    "import datasets\n",
    "from transformers import EncoderDecoderModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertConfig, EncoderDecoderConfig,EncoderDecoderModel\n",
    "from transformers import TrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict\n",
    "import joblib\n",
    "from transformers import PreTrainedTokenizer\n",
    "from tqdm import tqdm\n",
    "os.environ['HF_DATASETS_CACHE'] = \"~/temp/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccdaa1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05594d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
    "                         AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         TrainingArguments, BeamScorer, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5500c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "#######load tokenizer\n",
    "print(\"Load Data\")\n",
    "\n",
    "full_data = datasets.load_from_disk(\"/home/thienchuaheadshot_gmail_com/spin_content/data_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac13f1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'Giày búp bê mũi tròn',\n",
       " 'name': 'GIẦY BÚP BÊ NỮ ALDO BRAYLYNN',\n",
       " 'brand': 'ALDO',\n",
       " 'text': 'thương hiệu thời trang aldo là hãng thời trang canada chuyên mang đến những thiết kế sang trọng và tinh tế về phụ kiện giầy dép và túi xách. aldo được phân phối độc quyền tại việt nam bởi công ty cổ phần nhà thái từ năm 2009. túi và phụ kiện aldo không có hộp và túi vải. cần hỗ trợ tư vấn, vui lòng inbox nhân viên cskh của aldo.',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6071bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load Tokenizer\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"NlpHUST/gpt2-vietnamese\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"NlpHUST/gpt2-vietnamese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1435424b",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1b37f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNFREEZE_LAST_N = 6\n",
    "\n",
    "\n",
    "MODEL = \"NlpHUST/gpt2-vietnamese\"\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "TRAIN_SIZE      = 0.8\n",
    "TRAIN_BATCHSIZE = 32\n",
    "BATCH_UPDATE    = 64\n",
    "\n",
    "\n",
    "MAXLEN          = 768    #{768, 1024, 1280, 1600}\n",
    "EPOCHS          = 4\n",
    "LR              = 5e-4\n",
    "EPS             = 1e-8\n",
    "WARMUP_STEPS    = 1000\n",
    "\n",
    "SEED            = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8bc2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aaab934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenier(special_tokens=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
    "\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "\n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = AutoConfig.from_pretrained(MODEL, \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        #model = AutoModel.from_pretrained(load_model_path)\n",
    "        model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ad6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                #   load_model_path='pytorch_model.bin'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30fd46f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"NlpHUST/gpt2-vietnamese\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50258,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50259,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"pad_token_id\": 50261,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"sep_token_id\": 50262,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50263\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cfce7c",
   "metadata": {},
   "source": [
    "# Data and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4eb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [{\"text\":\"Đây là sản phẩm giày cao gót đi vừa êm vừa sướng, đi rất là tôn giáng đó!\", \"name\": 'GIẦY BÚP BÊ NỮ ALDO BRAYLYNN'},\n",
    "     {\"text\":\"Giày này không bán em ơi!\", \"name\": 'GIẦY BÚP BÊ NỮ NVIDIA'}]\n",
    "names = [x['name'] for x in batch]\n",
    "texts = [x['text'] for x in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "868dbb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([ SPECIAL_TOKENS['bos_token'] + name + SPECIAL_TOKENS['sep_token'] + text + SPECIAL_TOKENS['eos_token'] for name,text in zip(names,texts)], padding = \"max_length\", truncation=True, max_length=MAXLEN,return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2986ed76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50258, 10858, 23487, ..., 50261, 50261, 50261],\n",
       "       [50258, 10858, 23487, ..., 50261, 50261, 50261]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f6e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_dict = tokenizer(input,                                   \n",
    "                            truncation=True, \n",
    "                            max_length=MAXLEN, \n",
    "                            padding=\"max_length\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a19a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50258, 10858, 23487, 373, 29662, 39552, 20662, 12771, 19251, 8759, 5626, 28579, 3933, 50262, 2424, 314, 578, 659, 2664, 592, 6063, 585, 959, 3751, 959, 4056, 16, 585, 595, 314, 1971, 6658, 467, 5, 50259, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261, 50261], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe61dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):                                                               \n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]  \n",
    "    names = batch['name']\n",
    "    texts = batch['text']\n",
    "    inputs = tokenizer([ SPECIAL_TOKENS['bos_token'] + name + SPECIAL_TOKENS['sep_token'] + text + SPECIAL_TOKENS['eos_token'] for name,text in zip(names,texts)], padding = \"max_length\", truncation=True, max_length=MAXLEN,return_tensors=\"np\")\n",
    "    batch[\"labels\"] = inputs.input_ids \n",
    "    batch[\"input_ids\"] = inputs.input_ids                                                               \n",
    "    batch[\"attention_mask\"] = inputs.attention_mask                                                                                                                                           \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bba06fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/thienchuaheadshot_gmail_com/spin_content/data_clean/cache-ab795d76a411af75.arrow\n"
     ]
    }
   ],
   "source": [
    "full_data = full_data.filter(lambda x: len(x[\"text\"]) >8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "918d10cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['category', 'name', 'brand', 'text', 'id'],\n",
       "    num_rows: 40554\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba12dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbe65411b254ea6b76b69da54a5a490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653afe08461548f89422a12d6048adc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a2396572824bed9d9740b805c93efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5baf8825bd46c5abb8a161a729f4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7cc56541c435d9432f331d183c90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c81b572ca6f4ae7b94649cf9a761656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bde740f63e46febbbac0b07bea3087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303eac06f5f244c498a698065783a4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28d8d02475845e3bb3d8c4a20414e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#8:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fec2e1f499948719bb6256fd9d005e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#9:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = full_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "#     batch_size=1000, \n",
    "    remove_columns=[\"category\", \"name\",'text','id'],num_proc=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d33499a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.remove_columns(['brand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3df54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Freeze selective layers:\n",
    "# - Freeze all layers except last n:\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    #Only un-freeze the last n transformer blocks\n",
    "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78f017a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/thienchuaheadshot_gmail_com/spin_content/data_clean/cache-576a95d1249aa1d1.arrow and /home/thienchuaheadshot_gmail_com/spin_content/data_clean/cache-e5519ab616a2556c.arrow\n"
     ]
    }
   ],
   "source": [
    "data_full_split = train_data.train_test_split(test_size=0.05, seed = 42)\n",
    "train_data = data_full_split[\"train\"]                                    \n",
    "test_data = data_full_split[\"test\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f17b2a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2028\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a108e9aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.32 s, sys: 4.14 s, total: 8.47 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 19:51:28.885462: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"TPURoundRobin\" device_type: \"CPU\"') for unknown op: TPURoundRobin\n",
      "2022-07-28 19:51:28.886266: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"TpuHandleToProtoKey\" device_type: \"CPU\"') for unknown op: TpuHandleToProtoKey\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
    "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,\n",
    "    prediction_loss_only=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy='steps',\n",
    "    logging_steps=400,  # set to 1000 for full training\n",
    "    save_steps=1000,  # set to 500 for full training\n",
    "    eval_steps=1000,  # set to 8000 for full training\n",
    "    warmup_steps=WARMUP_STEPS,    \n",
    "    learning_rate=LR,\n",
    "    adam_epsilon=EPS,\n",
    "    weight_decay=0.01,        \n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da1b9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61097171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 38526\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2048\n",
      "  Gradient Accumulation steps = 64\n",
      "  Total optimization steps = 72\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='59' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [59/72 1:06:17 < 15:07, 0.01 it/s, Epoch 3.21/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c085487",
   "metadata": {},
   "source": [
    "### Generating text with finetune GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c97d0923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/vocab.json from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/7cf88785b592f4177f364d0c49db894e50b5c1c4f2a4af70dc87fe3ea5296b96.1c1d29c8ea56bae456d84e19ab8005ee46986b2f1cea420bf0ee5b6f37bc5447\n",
      "loading file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/merges.txt from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/d604f49ddd467aab72674d3a6fe91d48977020205ee65fee37edbc3793c57331.a70df4dd5bfb05f5ad6646b40581db340b74b8771c707fe89ce64b8fbf08d031\n",
      "loading file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/tokenizer.json from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/dcc1ad091eed0791d69c3b1013e546d77d35549c8c6d10316917785e2ff0cfad.61e940ecbaf4ec304b3aab441208b8185502600673c69ddc01d444307febb866\n",
      "loading file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/added_tokens.json from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/2e0c13b05c30253be7b68694dea18a31c1bd3503117627453a7cd8c09e0715a4.96c5d6ee3b47c84194a9df186239e69d3987f151b8b35ff90f003aa2a2d14f64\n",
      "loading file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/special_tokens_map.json from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/c9c91c3ccb451237f818e56b4076a7fdfcb1a4013c33ea6549bce9b37087f8e7.3ae9ae72462581d20e36bc528e9c47bb30cd671bb21add40ca0b24a0be9fac22\n",
      "loading file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/tokenizer_config.json from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/272b79710765d87e517714c86140ab1343667227222c0af3ed7d0ffdccdf4e26.124ac535cdb0e7f824591ae01c64124278c75ae3aeee7e61e8687f7a9271800c\n",
      "Assigning <|BOS|> to the bos_token key of the tokenizer\n",
      "Assigning <|EOS|> to the eos_token key of the tokenizer\n",
      "Assigning <|UNK|> to the unk_token key of the tokenizer\n",
      "Assigning <|PAD|> to the pad_token key of the tokenizer\n",
      "Assigning <|SEP|> to the sep_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/config.json from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/797cd60ee07bb6033508bbfe7156425d229228ae730e6661005b3c85fe55dd3b.c7ce1289c7853d140ef65348be24ee2e0577490b793b1e7fcbb88e3b81737f53\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"NlpHUST/gpt2-vietnamese\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 50258,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 50259,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50261,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"sep_token_id\": 50262,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/NlpHUST/gpt2-vietnamese/resolve/main/pytorch_model.bin from cache at /home/thienchuaheadshot_gmail_com/.cache/huggingface/transformers/5707161ef484d586632eb52af4e8b69eb19207bd5e7349a83206a98eaf998b61.7247682122ce75b7d91426ef29523457a08597d3ba244163b7f26c83816a583c\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at NlpHUST/gpt2-vietnamese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                  load_model_path='./model/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3acaefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4fda92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Giày Quốc Việt: \"\n",
    "des = \"Sản phẩm SM05 là chiếc giày sang trọng của hãng Quốc Việt. Với thiết kế thời trang và độc đáo, bạn có thể mang theo bên mình mọi lúc trong những chuyến đi chơi xa hoặc các hoạt động vui nhộn mà không cần lo lắng về vấn đề an toàn sức khỏe khi sử dụng sản phầm này nữa nhé. Mẫu mã đẹp mắt cùng nhiều ưu điểm nổi bật khác được làm từ chất liệu da bò thật 100% đã tạo nên sự cuốn hút khó cưỡng đối với người nhìn ngay lần đầu tiên ngắm nghía mẫu giầy cao cấp chính hiệu quốc việt tại shop chúng tôi.\"\n",
    "\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + name + SPECIAL_TOKENS['sep_token'] + des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "303f41ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|BOS|>Giày Quốc Việt: <|SEP|>Sản phẩm SM05 là chiếc giày sang trọng của hãng Quốc Việt. Với thiết kế thời trang và độc đáo, bạn có thể mang theo bên mình mọi lúc trong những chuyến đi chơi xa hoặc các hoạt động vui nhộn mà không cần lo lắng về vấn đề an toàn sức khỏe khi sử dụng sản phầm này nữa nhé. Mẫu mã đẹp mắt cùng nhiều ưu điểm nổi bật khác được làm từ chất liệu da bò thật 100% đã tạo nên sự cuốn hút khó cưỡng đối với người nhìn ngay lần đầu tiên ngắm nghía mẫu giầy cao cấp chính hiệu quốc việt tại shop chúng tôi.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7becf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cd7ab9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50258, 12610,   883,   649,    30,   225, 50262,  4147,   659,  8982,\n",
       "          3309,   314,  1076,  2664,  1218,   828,   331,  1520,   883,   649,\n",
       "            18,  1470,   695,   795,   606,   840,   309,  1295,  2039,    16,\n",
       "           430,   329,   419,   836,   608,   909,   589,   962,  1036,   366,\n",
       "           392,  2025,   585,  1092,  1353,   743,   338,   876,   535,  1252,\n",
       "          5349,   481,   359,   675,   565,  2034,   473,   889,   600,   841,\n",
       "           710,   967,  1334,   456,   603,   515,   578, 14758,   444,  1078,\n",
       "          1404,    18,  4206,  1692,   805,  1012,   648,   480,  1370,   738,\n",
       "          1179,  1838,   474,   354,   471,   450,   668,   936,   904,  2647,\n",
       "          1086,  1600,     9,   417,   707,   588,   508,  2026,  1444,   983,\n",
       "          4668,   729,   370,   390,  1103,   987,   897,   524,  1071,  3175,\n",
       "         17372,  1261,  9196,   592,   656,   577,   745,   921,  2607,   491,\n",
       "          3796,   634,   554,    18]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1730b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "20eb6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-p (nucleus) text generation (10 samples):\n",
    "sample_outputs = model.generate(generated,do_sample=True, min_length=20, max_length=300,top_k=30,top_p=0.7,temperature=0.9,repetition_penalty=2.0,num_return_sequences=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "211c0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Giày Quốc Việt: Sản phẩm SM05 là chiếc giày sang trọng của hãng Quốc Việt. Với thiết kế thời trang và độc đáo, bạn có thể mang theo bên mình mọi lúc trong những chuyến đi chơi xa hoặc các hoạt động vui nhộn mà không cần lo lắng về vấn đề an toàn sức khỏe khi sử dụng sản phầm này nữa nhé. Mẫu mã đẹp mắt cùng nhiều ưu điểm nổi bật khác được làm từ chất liệu da bò thật 100% đã tạo nên sự cuốn hút khó cưỡng đối với người nhìn ngay lần đầu tiên ngắm nghía mẫu giầy cao cấp chính hiệu quốc việt tại shop chúng tôi.\n",
      "Với kiểu dáng đơn giản nhưng lại vô cũng hiện đại phù hợp cho cả nam lẫn nữ. Sản Phẩm Giày Thể Thao Nữ Thời Trang Chính Hãng sẽ giúp ích rất lớn để bảo vệ đôi chân luôn thoải mái nhất. Bạn chỉ mất khoảng 1 giờ đồng hồ mỗi ngày là hoàn thành việc mua sắm đồ dùng gia đình rồi đấy!Một số công trình giao thông trên địa bàn TP Đà Nẵng (TPĐN) do Ban quản lý dự án đường bộ Thăng Long - Sông Hàn thực tế chưa triển khai thi công đúng tiến độ như cam kết ban đâu Tư nhân hoá tuyến tránh Hải Vân; xây dựng hệ thống hầm chui nút Túy Loan...\tRead more →Trang chủ » Tin tức, tin-tuc & căn hộ đẳng cấp | Căn hộ Palm Garden Keppel Land còn đang lãng mảnh tại khu vự: Q2\n",
      "\n",
      "\n",
      "2: Giày Quốc Việt: Sản phẩm SM05 là chiếc giày sang trọng của hãng Quốc Việt. Với thiết kế thời trang và độc đáo, bạn có thể mang theo bên mình mọi lúc trong những chuyến đi chơi xa hoặc các hoạt động vui nhộn mà không cần lo lắng về vấn đề an toàn sức khỏe khi sử dụng sản phầm này nữa nhé. Mẫu mã đẹp mắt cùng nhiều ưu điểm nổi bật khác được làm từ chất liệu da bò thật 100% đã tạo nên sự cuốn hút khó cưỡng đối với người nhìn ngay lần đầu tiên ngắm nghía mẫu giầy cao cấp chính hiệu quốc việt tại shop chúng tôi.\n",
      "Mẫu Giày Thể Thao Chính Hãng Mới Nhất 2018-2019 | Mua Bán Quảng Nam - Rao Vặt quảng nam - Diễn đàn du lịch Đà Nẵng HuếQuảng cáo trên báo đài – một loại hình kinh doanh rất phổ biến hiện nay đang phát triển mạnh mẽ ở nước ta cũng như khu vực Đông Bắc Á nói chung. Việc sở hữu cho công ty hay đơn vị cá nhân 1 ấn bản in tờ rơi thì nó càng quan trọng hơn cả việc đưa thông tin đến khách hàng. Tuy nhiên vì lý do nào đó họ muốn truyền tải hết nội dung của tài nguyên tới tất thảy cộng đồng qua biển số xe ô tô, máy bay… Vậy để biết cách thu hồi nợ xấu hãy đọc bài viết dưới đây của Công Ty Luật TNHH Sao Kim sẽ giúp quý Khách trả lời câu hỏi trên.(Kiến Thức) – Những năm gần đây con đường bê tông nông thôn mới (\n",
      "\n",
      "\n",
      "3: Giày Quốc Việt: Sản phẩm SM05 là chiếc giày sang trọng của hãng Quốc Việt. Với thiết kế thời trang và độc đáo, bạn có thể mang theo bên mình mọi lúc trong những chuyến đi chơi xa hoặc các hoạt động vui nhộn mà không cần lo lắng về vấn đề an toàn sức khỏe khi sử dụng sản phầm này nữa nhé. Mẫu mã đẹp mắt cùng nhiều ưu điểm nổi bật khác được làm từ chất liệu da bò thật 100% đã tạo nên sự cuốn hút khó cưỡng đối với người nhìn ngay lần đầu tiên ngắm nghía mẫu giầy cao cấp chính hiệu quốc việt tại shop chúng tôi.\n",
      "Mẫu dép nam màu đen cổ điển luôn chiếm lĩnh vị trí hàng đầu so với tất cả mọi loại đồ dùng phục vụ cho cuộc sống hiện đại ngày nay như điện thoại di dộng hay laptop... Vậy để sở hữu một đôi chân thon gọn hãy đến Shop Giày Thể Thao Nam Đẹp Nhất Sài Gòn. Bạn sẽ chọn lựa rất đơn giản chỉ bằng cách click vào đường link sau đây : https://giaydantuongdepnhatsaigon.com/shop-bancaosneu2a4b5f8ca6e1_njw3y0m7iOqPpBkN9QAJoExZXvYzTtWgIngL+UrVqqMHlK4F(VTC News) - Các nhà khoa học Mỹ vừa phát triển thành công thuốc kháng ung thư tự\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)   \n",
    "    print(\"{}: {}\\n\\n\".format(i+1, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af94bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
